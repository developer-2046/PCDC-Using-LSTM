{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GRU Implementation of PCDCnet"
      ],
      "metadata": {
        "id": "ChHS8jzJGrZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch pandas numpy scikit-learn torch_geometric"
      ],
      "metadata": {
        "id": "3YC_fmUYGxwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3hDrqR1HKbW",
        "outputId": "6bc80575-6abd-4081-a211-e9283a43f7a6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Pre-processing"
      ],
      "metadata": {
        "id": "Hc5nEEGDHGKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os # For constructing file paths\n",
        "\n",
        "# --- Helper Function to Load and Preprocess Single Yearly Meteorological Files ---\n",
        "def load_single_met_file(file_path, met_var_name, relevant_col_name, is_hourly):\n",
        "    \"\"\"\n",
        "    Loads a single yearly meteorological CSV, standardizes columns,\n",
        "    and performs daily aggregation if the data is hourly.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Use low_memory=False to handle potentially mixed types in large CSVs\n",
        "        df = pd.read_csv(file_path, low_memory=False)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: File not found {file_path}. Skipping this file.\")\n",
        "        return None\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(f\"Warning: File is empty {file_path}. Skipping this file.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "    # Standardize column names (assuming yearly files might use different casing)\n",
        "    # Create a mapping based on common variations\n",
        "    column_rename_map = {}\n",
        "    # Date column\n",
        "    if 'DATE' in df.columns: column_rename_map['DATE'] = 'Date'\n",
        "    elif 'date' in df.columns: column_rename_map['date'] = 'Date'\n",
        "    # Latitude column\n",
        "    if 'LATITUDE' in df.columns: column_rename_map['LATITUDE'] = 'Latitude'\n",
        "    elif 'latitude' in df.columns: column_rename_map['latitude'] = 'Latitude'\n",
        "    # Longitude column\n",
        "    if 'LONGITUDE' in df.columns: column_rename_map['LONGITUDE'] = 'Longitude'\n",
        "    elif 'longitude' in df.columns: column_rename_map['longitude'] = 'Longitude'\n",
        "    # Value column\n",
        "    if relevant_col_name in df.columns: column_rename_map[relevant_col_name] = met_var_name\n",
        "\n",
        "    df.rename(columns=column_rename_map, inplace=True)\n",
        "\n",
        "    # Check if essential columns are present after renaming\n",
        "    required_cols = ['Date', 'Latitude', 'Longitude', met_var_name]\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        print(f\"Warning: Missing one or more required columns ({required_cols}) in {file_path} after renaming. Skipping.\")\n",
        "        # print(f\"Available columns: {df.columns.tolist()}\")\n",
        "        return None\n",
        "\n",
        "    # Ensure correct data types and handle errors by coercing to NaN\n",
        "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "    df['Latitude'] = pd.to_numeric(df['Latitude'], errors='coerce')\n",
        "    df['Longitude'] = pd.to_numeric(df['Longitude'], errors='coerce')\n",
        "    df[met_var_name] = pd.to_numeric(df[met_var_name], errors='coerce')\n",
        "\n",
        "    # Drop rows where essential identifiers or the value itself are NaN after conversion\n",
        "    df.dropna(subset=['Date', 'Latitude', 'Longitude', met_var_name], inplace=True)\n",
        "\n",
        "    if df.empty:\n",
        "        # print(f\"Warning: No valid data left in {file_path} after cleaning. Skipping.\")\n",
        "        return None\n",
        "\n",
        "    if is_hourly:\n",
        "        # Aggregate to daily. For Wind Direction, simple mean of degrees is a placeholder.\n",
        "        # A more accurate method for wind direction involves vector averaging.\n",
        "        df_daily = df.groupby(['Date', 'Latitude', 'Longitude'], as_index=False)[met_var_name].mean()\n",
        "        return df_daily\n",
        "    else:\n",
        "        # If data is already daily, select relevant columns and remove duplicates if any\n",
        "        df_daily = df[['Date', 'Latitude', 'Longitude', met_var_name]].drop_duplicates(subset=['Date', 'Latitude', 'Longitude'])\n",
        "        return df_daily\n",
        "\n",
        "# --- Function to Load and Merge All Pollutant and Meteorological Data ---\n",
        "def load_and_merge_all_data(main_pollutant_file, yearly_data_directory, train_years_range):\n",
        "    \"\"\"\n",
        "    Loads the main pollutant data and merges it with yearly meteorological data\n",
        "    for the specified training years.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Data Loading and Merging ---\")\n",
        "    # Load main pollutant data\n",
        "    try:\n",
        "        df_pollutants = pd.read_csv(main_pollutant_file)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Main pollutant file {main_pollutant_file} not found.\")\n",
        "        return None\n",
        "\n",
        "    df_pollutants['Date'] = pd.to_datetime(df_pollutants['Date'])\n",
        "\n",
        "    # Select relevant pollutant columns from the main file\n",
        "    pollutant_cols = ['Date', 'Latitude', 'Longitude',\n",
        "                      'Daily Mean PM2.5 Concentration',\n",
        "                      'Daily Max 1-hour NO2 Concentration', # Using Max as per file\n",
        "                      'Daily Max 1-hour SO2 Concentration'] # Using Max as per file\n",
        "\n",
        "    # Check if all pollutant columns exist\n",
        "    missing_poll_cols = [col for col in pollutant_cols if col not in df_pollutants.columns]\n",
        "    if missing_poll_cols:\n",
        "        print(f\"Error: Missing required pollutant columns in {main_pollutant_file}: {missing_poll_cols}\")\n",
        "        return None\n",
        "    df_pollutants = df_pollutants[pollutant_cols]\n",
        "\n",
        "    # Filter pollutant data for the specified training years\n",
        "    df_merged = df_pollutants[df_pollutants['Date'].dt.year.isin(train_years_range)].copy()\n",
        "    if df_merged.empty:\n",
        "        print(f\"Warning: No pollutant data found for the years {train_years_range} in {main_pollutant_file}.\")\n",
        "        # If no base data, we can't proceed with merging\n",
        "        return pd.DataFrame() # Return an empty DataFrame\n",
        "\n",
        "    print(f\"Loaded pollutant data for {len(df_merged)} records within {train_years_range}.\")\n",
        "\n",
        "    # Configuration for meteorological variables\n",
        "    met_vars_config = {\n",
        "        # New Name      File Suffix            Original Column Name in CSV         Is Hourly?\n",
        "        'Temperature': {'suffix': 'Temperature.csv', 'col': 'DailyAverageDryBulbTemperature', 'hourly': False},\n",
        "        'WindDir':     {'suffix': 'WindDir.csv',     'col': 'HourlyWindDirection',            'hourly': True},\n",
        "        'WindSpeed':   {'suffix': 'WindSpeed.csv',   'col': 'HourlyWindSpeed',                'hourly': True},\n",
        "    }\n",
        "\n",
        "    # Round Latitude and Longitude in the base merged df for robust merging\n",
        "    df_merged['Latitude'] = df_merged['Latitude'].round(4)\n",
        "    df_merged['Longitude'] = df_merged['Longitude'].round(4)\n",
        "\n",
        "    for met_var_new_name, config in met_vars_config.items():\n",
        "        all_years_single_met_df_list = []\n",
        "        print(f\"\\nProcessing meteorological variable: {met_var_new_name}\")\n",
        "        for year in train_years_range:\n",
        "            # Construct file path assuming files are named like \"YYYY-Suffix.csv\"\n",
        "            file_name = f\"{year}-{config['suffix']}\"\n",
        "            file_path = os.path.join(yearly_data_directory, file_name)\n",
        "\n",
        "            # print(f\"Attempting to load: {file_path}\")\n",
        "            df_year_met = load_single_met_file(file_path, met_var_new_name, config['col'], config['hourly'])\n",
        "            if df_year_met is not None and not df_year_met.empty:\n",
        "                all_years_single_met_df_list.append(df_year_met)\n",
        "            # else:\n",
        "                # print(f\"No data loaded for {met_var_new_name} for year {year}.\")\n",
        "\n",
        "        if all_years_single_met_df_list:\n",
        "            df_all_years_single_met = pd.concat(all_years_single_met_df_list, ignore_index=True)\n",
        "            print(f\"Successfully concatenated {met_var_new_name} data for all years ({len(df_all_years_single_met)} records).\")\n",
        "\n",
        "            # Round lat/lon for merging\n",
        "            df_all_years_single_met['Latitude'] = df_all_years_single_met['Latitude'].round(4)\n",
        "            df_all_years_single_met['Longitude'] = df_all_years_single_met['Longitude'].round(4)\n",
        "\n",
        "            # Perform the merge\n",
        "            df_merged = pd.merge(df_merged, df_all_years_single_met,\n",
        "                                 on=['Date', 'Latitude', 'Longitude'],\n",
        "                                 how='left') # Left join to keep all pollutant records\n",
        "            print(f\"Merged {met_var_new_name}. df_merged now has {len(df_merged)} records.\")\n",
        "        else:\n",
        "            print(f\"Warning: No data successfully loaded for {met_var_new_name} across any specified years. Column will be NaNs.\")\n",
        "            df_merged[met_var_new_name] = np.nan # Add an empty column\n",
        "\n",
        "    print(\"--- Data Loading and Merging Complete ---\")\n",
        "    return df_merged\n",
        "\n",
        "\n",
        "# --- Main Preprocessing Function to Create Model Inputs ---\n",
        "def preprocess_and_create_model_inputs(\n",
        "    merged_df,\n",
        "    target_pollutant_col,\n",
        "    feature_cols_to_use, # Initial list of features from merged_df to consider\n",
        "    completeness_threshold=0.7, # Minimum % of data a station must have\n",
        "    n_neighbors=8, # For GCN graph\n",
        "    lookback=7     # Timesteps to look back\n",
        "):\n",
        "    \"\"\"\n",
        "    Processes the merged dataframe: station reliability, pivot, zero-variance removal,\n",
        "    NaN filling, scaling, and sequence creation.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Data Preprocessing and Input Creation ---\")\n",
        "    if merged_df is None or merged_df.empty:\n",
        "        print(\"Error: Input DataFrame is None or empty. Cannot preprocess.\")\n",
        "        return None, None, None, None, None, None, None, None\n",
        "\n",
        "    df = merged_df.copy()\n",
        "\n",
        "    # Ensure feature_cols_to_use actually exist in df\n",
        "    actual_features_present = [col for col in feature_cols_to_use if col in df.columns]\n",
        "    if len(actual_features_present) != len(feature_cols_to_use):\n",
        "        missing_features = [col for col in feature_cols_to_use if col not in df.columns]\n",
        "        print(f\"Warning: Some specified features are not in the merged DataFrame: {missing_features}\")\n",
        "        print(f\"Using available features: {actual_features_present}\")\n",
        "    feature_cols_to_use = actual_features_present\n",
        "    if not feature_cols_to_use or target_pollutant_col not in feature_cols_to_use:\n",
        "        print(f\"Error: Target pollutant '{target_pollutant_col}' or other essential features are missing from the merged data.\")\n",
        "        return None, None, None, None, None, None, None, None\n",
        "\n",
        "\n",
        "    # 1. Station Reliability Check\n",
        "    # Create a unique station identifier string for grouping (handles float precision)\n",
        "    df['station_uid_str'] = df['Latitude'].round(4).astype(str) + '_' + df['Longitude'].round(4).astype(str)\n",
        "\n",
        "    unique_dates_in_period = df['Date'].unique()\n",
        "    total_days_in_period = len(unique_dates_in_period)\n",
        "\n",
        "    if total_days_in_period == 0:\n",
        "        print(\"Error: No unique dates found in the data. Cannot perform reliability check.\")\n",
        "        return None, None, None, None, None, None, None, None\n",
        "\n",
        "    # Calculate completeness based on the target pollutant\n",
        "    station_completeness = df.groupby('station_uid_str')[target_pollutant_col].count() / total_days_in_period\n",
        "    reliable_station_uids = station_completeness[station_completeness >= completeness_threshold].index.tolist()\n",
        "\n",
        "    if not reliable_station_uids:\n",
        "        raise ValueError(f\"No stations meet the completeness threshold of {completeness_threshold*100}%. Try lowering it or check data merging.\")\n",
        "\n",
        "    print(f\"--- Station Reliability Check (Threshold: {completeness_threshold*100}%) ---\")\n",
        "    print(f\"Total unique station UIDs found before filtering: {df['station_uid_str'].nunique()}\")\n",
        "    print(f\"Keeping {len(reliable_station_uids)} reliable stations.\")\n",
        "\n",
        "    df = df[df['station_uid_str'].isin(reliable_station_uids)].copy()\n",
        "    if df.empty:\n",
        "        print(\"Error: No data left after station reliability filtering.\")\n",
        "        return None, None, None, None, None, None, None, None\n",
        "\n",
        "    # Create a final integer `location_id` for the reliable stations\n",
        "    locations_df = df[['Latitude', 'Longitude']].drop_duplicates().reset_index(drop=True)\n",
        "    locations_df['location_id'] = locations_df.index\n",
        "\n",
        "    # Merge this integer location_id back\n",
        "    df = pd.merge(df, locations_df, on=['Latitude', 'Longitude'], how='left')\n",
        "    print(f\"Data reduced to {len(df)} records after reliability filtering and location_id mapping.\")\n",
        "\n",
        "    # 2. Pivot Data\n",
        "    # Pivoting with `location_id` as columns and `Date` as index. Values are the features.\n",
        "    try:\n",
        "        data_pivot = df.pivot_table(index='Date', columns='location_id',\n",
        "                                    values=feature_cols_to_use, aggfunc='mean')\n",
        "    except Exception as e:\n",
        "        print(f\"Error during pivot: {e}\")\n",
        "        print(\"Check for duplicate (Date, location_id) entries or issues with feature_cols_to_use.\")\n",
        "        # print(df[df.duplicated(subset=['Date', 'location_id'], keep=False)])\n",
        "        return None, None, None, None, None, None, None, None\n",
        "\n",
        "    print(f\"Pivoted data shape: {data_pivot.shape}\")\n",
        "\n",
        "\n",
        "    # 3. Zero-Variance Feature Removal (applied to pivoted data)\n",
        "    # Columns in data_pivot are MultiIndex: (feature_name, location_id)\n",
        "    # We check variance for each base feature_name across all its locations and times.\n",
        "    current_features_in_pivot = data_pivot.columns.get_level_values(0).unique().tolist()\n",
        "    final_feature_cols_for_model = list(current_features_in_pivot) # Start with all pivoted features\n",
        "\n",
        "    print(\"\\n--- Checking for Zero-Variance Features Post-Pivot ---\")\n",
        "    for feature_name in current_features_in_pivot:\n",
        "        feature_data_matrix = data_pivot[feature_name] # This selects all location columns for that feature\n",
        "        # Check if the entire block of data for this feature (all its locations over all time) has zero variance\n",
        "        if feature_data_matrix.to_numpy(na_value=np.nan).var() == 0: # Use nanvar if NaNs are expected\n",
        "            print(f\"ðŸš© Found zero-variance feature post-pivot: '{feature_name}'. It will be removed.\")\n",
        "            if feature_name in final_feature_cols_for_model:\n",
        "                final_feature_cols_for_model.remove(feature_name)\n",
        "            # Drop from pivot table\n",
        "            data_pivot = data_pivot.drop(columns=feature_name, level=0)\n",
        "\n",
        "    if target_pollutant_col not in final_feature_cols_for_model:\n",
        "        raise ValueError(f\"Target pollutant '{target_pollutant_col}' was removed due to zero variance or missing. Cannot proceed.\")\n",
        "    print(f\"âœ… Final features for model: {final_feature_cols_for_model}\\n\")\n",
        "\n",
        "\n",
        "    # 4. Fill NaNs (Crucial after pivoting)\n",
        "    # Sort columns to ensure ffill/bfill are predictable (feature then location_id)\n",
        "    data_pivot = data_pivot.sort_index(axis=1)\n",
        "\n",
        "    # Fill across time first (axis=0) for each (feature, location) series\n",
        "    data_pivot.ffill(axis=0, inplace=True)\n",
        "    data_pivot.bfill(axis=0, inplace=True)\n",
        "\n",
        "    # Interpolate for any remaining gaps in time series\n",
        "    data_pivot.interpolate(method='linear', axis=0, limit_direction='both', inplace=True)\n",
        "\n",
        "    # If NaNs still exist (e.g., a station has NO data for a feature over the entire period),\n",
        "    # fill them. Filling with 0 is a last resort. Mean of the feature could be better.\n",
        "    if data_pivot.isnull().values.any():\n",
        "        print(\"Warning: Data still contains NaNs after ffill/bfill/interpolate. Filling remaining with 0.\")\n",
        "        data_pivot.fillna(0, inplace=True)\n",
        "\n",
        "\n",
        "    # 5. Normalize Data\n",
        "    # The data_pivot columns are MultiIndex (feature_name, location_id).\n",
        "    # The scaler will learn a mean/std for each of these individual series.\n",
        "    scaler = StandardScaler()\n",
        "    scaled_values = scaler.fit_transform(data_pivot.to_numpy())\n",
        "    df_scaled_pivot = pd.DataFrame(scaled_values, index=data_pivot.index, columns=data_pivot.columns)\n",
        "\n",
        "    # 6. Reshape Scaled Data to 3D Tensor: (num_dates, num_locations, num_features)\n",
        "    num_dates = len(df_scaled_pivot.index)\n",
        "    num_reliable_locations = len(locations_df) # Based on reliable stations\n",
        "    num_final_model_features = len(final_feature_cols_for_model)\n",
        "\n",
        "    final_3d_data_np = np.zeros((num_dates, num_reliable_locations, num_final_model_features))\n",
        "\n",
        "    for i, feature_name_model in enumerate(final_feature_cols_for_model):\n",
        "        for j, loc_id_model in enumerate(locations_df['location_id']): # loc_id_model is 0, 1, ..., N-1\n",
        "            # The column in df_scaled_pivot is (feature_name_model, loc_id_model)\n",
        "            if (feature_name_model, loc_id_model) in df_scaled_pivot.columns:\n",
        "                final_3d_data_np[:, j, i] = df_scaled_pivot[(feature_name_model, loc_id_model)].values\n",
        "            else:\n",
        "                # This might happen if a feature was dropped for a specific location due to all NaNs,\n",
        "                # or if a feature was entirely dropped (handled by zero-var check).\n",
        "                # Should be less common with robust NaN filling.\n",
        "                print(f\"Warning: Column ({feature_name_model}, {loc_id_model}) not found in scaled pivot during 3D array construction. Filling with 0.\")\n",
        "                final_3d_data_np[:, j, i] = 0 # Or some other imputation\n",
        "\n",
        "    # 7. Create Graph Structure and Input/Output Sequences\n",
        "    adj_matrix = kneighbors_graph(locations_df[['Latitude', 'Longitude']],\n",
        "                                  n_neighbors=min(n_neighbors, num_reliable_locations -1 if num_reliable_locations >1 else 1), # n_neighbors can't be > n_points-1\n",
        "                                  mode='connectivity', include_self=True)\n",
        "    edge_index = torch.tensor(np.array(adj_matrix.nonzero()), dtype=torch.long)\n",
        "\n",
        "    X_list, y_list = [], []\n",
        "    try:\n",
        "        target_feature_idx_in_model = final_feature_cols_for_model.index(target_pollutant_col)\n",
        "    except ValueError:\n",
        "        raise ValueError(f\"Target pollutant '{target_pollutant_col}' not in final model features: {final_feature_cols_for_model}\")\n",
        "\n",
        "    for i in range(len(final_3d_data_np) - lookback):\n",
        "        X_list.append(final_3d_data_np[i : i + lookback, :, :])\n",
        "        y_list.append(final_3d_data_np[i + lookback, :, target_feature_idx_in_model])\n",
        "\n",
        "    if not X_list or not y_list:\n",
        "        print(\"Error: Not enough data to create lookback sequences. Consider a smaller lookback or more data.\")\n",
        "        return None, None, None, None, None, None, None, None\n",
        "\n",
        "    X_tensor = torch.tensor(np.array(X_list), dtype=torch.float32)\n",
        "    y_tensor = torch.tensor(np.array(y_list), dtype=torch.float32)\n",
        "\n",
        "    print(\"--- Preprocessing and Input Creation Complete ---\")\n",
        "    return (X_tensor, y_tensor, edge_index, scaler, locations_df,\n",
        "            num_reliable_locations, num_final_model_features, final_feature_cols_for_model)\n",
        "\n",
        "\n",
        "# --- Main Script Flow (Example) ---\n",
        "if __name__ == '__main__':\n",
        "    # Define paths and parameters\n",
        "    base_data_directory_path = '/content/drive/MyDrive/pcdc'\n",
        "    main_pollutant_file_path = 'Master99-23.csv' # Your main pollutant data\n",
        "    # IMPORTANT: Replace with the ACTUAL path to the directory containing yearly CSVs\n",
        "    # e.g., '/content/data/met_yearly_csvs/' or 'met_data/' if in the same directory\n",
        "    yearly_met_data_dir = '.' # Placeholder: current directory. Update this!\n",
        "\n",
        "    train_years = range(2010, 2023) # Train on 2010-2022\n",
        "\n",
        "    # Load and merge all data sources\n",
        "    df_merged_all_years = load_and_merge_all_data(main_pollutant_file_path,\n",
        "                                                  yearly_met_data_dir,\n",
        "                                                  train_years)\n",
        "\n",
        "    if df_merged_all_years is not None and not df_merged_all_years.empty:\n",
        "        print(f\"\\nTotal records in merged DataFrame for 2010-2022: {len(df_merged_all_years)}\")\n",
        "        print(f\"Columns in merged DataFrame: {df_merged_all_years.columns.tolist()}\")\n",
        "        # print(df_merged_all_years.head())\n",
        "        # print(df_merged_all_years.isnull().sum())\n",
        "\n",
        "\n",
        "        # Define features to use from the merged DataFrame\n",
        "        # These must match column names AFTER merging and any renaming in load_single_met_file\n",
        "        initial_features_for_model = [\n",
        "            'Daily Mean PM2.5 Concentration',\n",
        "            'Daily Max 1-hour NO2 Concentration',\n",
        "            'Daily Max 1-hour SO2 Concentration',\n",
        "            'Temperature',\n",
        "            'WindSpeed',\n",
        "            'WindDir'\n",
        "        ]\n",
        "        target_col = 'Daily Mean PM2.5 Concentration'\n",
        "\n",
        "        # Preprocess the merged data to create model inputs\n",
        "        processed_outputs = preprocess_and_create_model_inputs(\n",
        "            merged_df=df_merged_all_years,\n",
        "            target_pollutant_col=target_col,\n",
        "            feature_cols_to_use=initial_features_for_model,\n",
        "            completeness_threshold=0.6, # Adjusted threshold, can be tuned\n",
        "            n_neighbors=4,              # Can be tuned\n",
        "            lookback=7                  # Standard lookback period\n",
        "        )\n",
        "\n",
        "        if processed_outputs[0] is not None: # Check if X_tensor is not None\n",
        "            X, y, edge_idx, data_scaler, reliable_locs_df, \\\n",
        "            n_locs, n_feats, final_feats_list = processed_outputs\n",
        "\n",
        "            print(\"\\n--- Final Processed Data Shapes ---\")\n",
        "            print(f\"X shape: {X.shape}\")\n",
        "            print(f\"y shape: {y.shape}\")\n",
        "            print(f\"edge_index shape: {edge_idx.shape}\")\n",
        "            print(f\"Number of reliable locations: {n_locs}\")\n",
        "            print(f\"Number of features in model: {n_feats}\")\n",
        "            print(f\"Final features list: {final_feats_list}\")\n",
        "            print(f\"Reliable stations info:\\n{reliable_locs_df.head()}\")\n",
        "\n",
        "            # Now you can proceed to split X and y into train/val/test sets,\n",
        "            # create DataLoaders, define the PCDCNet model (adjusting input_dim if n_feats changed),\n",
        "            # and train the model.\n",
        "            # Example:\n",
        "            # train_size = int(0.8 * len(X))\n",
        "            # X_train, y_train = X[:train_size], y[:train_size]\n",
        "            # ... and so on for validation/test split\n",
        "\n",
        "            # Remember to pass n_locs and n_feats to your PCDCNet model instantiation.\n",
        "            # model = PCDCNet(num_features=n_feats, num_locations=n_locs)\n",
        "\n",
        "        else:\n",
        "            print(\"Failed to create model inputs due to errors in preprocessing.\")\n",
        "    else:\n",
        "        print(\"Failed to load and merge data. Cannot proceed with preprocessing.\")"
      ],
      "metadata": {
        "id": "yxps9H_3HwPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCDCNet Model Architecture"
      ],
      "metadata": {
        "id": "jykBnAxmIkcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. PCDCNet Model Definition ---\n",
        "# This is the model architecture you provided. It is a solid implementation\n",
        "# for this task.\n",
        "class PCDCNet(nn.Module):\n",
        "    def __init__(self, num_features, num_locations, hidden_dim=64, gcn_out_dim=32):\n",
        "        super(PCDCNet, self).__init__()\n",
        "\n",
        "        # MLP for local feature enhancement\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(num_features, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Graph Convolutional Network for spatial dependencies\n",
        "        self.gcn = GCNConv(hidden_dim, gcn_out_dim)\n",
        "\n",
        "        # GRU for temporal dependencies\n",
        "        self.gru = nn.GRU(gcn_out_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "        # Output layer\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # x shape: (batch_size, lookback, num_locations, num_features)\n",
        "        batch_size, lookback, num_loc, num_feat = x.shape\n",
        "\n",
        "        # Process each time step\n",
        "        gru_inputs = []\n",
        "        for t in range(lookback):\n",
        "            # Current time step data for all locations in the batch\n",
        "            x_t = x[:, t, :, :]\n",
        "            x_t = x_t.reshape(batch_size * num_loc, num_feat)\n",
        "\n",
        "            # Local feature enhancement with MLP\n",
        "            mlp_out = self.mlp(x_t)\n",
        "\n",
        "            # Reshape for GCN\n",
        "            mlp_out = mlp_out.reshape(batch_size, num_loc, -1)\n",
        "\n",
        "            # GCN for spatial modeling\n",
        "            # We process each graph in the batch individually\n",
        "            gcn_batch_out = []\n",
        "            for i in range(batch_size):\n",
        "                gcn_out = self.gcn(mlp_out[i], edge_index)\n",
        "                gcn_batch_out.append(gcn_out)\n",
        "\n",
        "            gcn_out_tensor = torch.stack(gcn_batch_out)\n",
        "            gru_inputs.append(gcn_out_tensor)\n",
        "\n",
        "        # Stack to create a sequence for the GRU\n",
        "        # Shape: (batch_size, lookback, num_loc, gcn_out_dim)\n",
        "        gru_input_seq = torch.stack(gru_inputs, dim=1)\n",
        "\n",
        "        # Reshape to combine batch and location dimensions for GRU processing\n",
        "        # The GRU will treat each location's time series independently\n",
        "        gru_input_seq = gru_input_seq.reshape(batch_size * num_loc, lookback, -1)\n",
        "\n",
        "        # GRU for temporal modeling\n",
        "        gru_out, _ = self.gru(gru_input_seq)\n",
        "\n",
        "        # Use the output of the last time step\n",
        "        last_hidden_state = gru_out[:, -1, :]\n",
        "\n",
        "        # Reshape back to (batch_size, num_loc, hidden_dim)\n",
        "        last_hidden_state = last_hidden_state.reshape(batch_size, num_loc, -1)\n",
        "\n",
        "        # Fully connected layer for the final prediction\n",
        "        output = self.fc(last_hidden_state)\n",
        "\n",
        "        return output.squeeze(-1)\n",
        "\n",
        "\n",
        "# --- 2. Model Training Function ---\n",
        "def train_model(model, train_loader, val_loader, edge_index, device, epochs=50, learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Trains the PCDCNet model with gradient clipping for stability.\n",
        "    \"\"\"\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    print(\"\\n--- Starting Model Training ---\")\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs, edge_index)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            if torch.isnan(loss):\n",
        "                print(f\"NaN detected in loss at epoch {epoch+1}. Stopping training.\")\n",
        "                return train_losses, val_losses\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Gradient clipping\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_running_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs, edge_index)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_running_loss += loss.item()\n",
        "\n",
        "        val_loss = val_running_loss / len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
        "\n",
        "    print(\"--- Model Training Complete ---\")\n",
        "    return train_losses, val_losses\n",
        "\n",
        "\n",
        "# --- 3. Model Evaluation Function ---\n",
        "def evaluate_model(model, test_loader, edge_index, device, scaler_obj, num_locations, num_features, target_feature_idx):\n",
        "    \"\"\"\n",
        "    Evaluates the model on the test set and returns metrics on the original data scale.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    print(\"\\n--- Evaluating Model on Test Set ---\")\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs, edge_index)\n",
        "            all_preds.append(outputs.cpu().numpy())\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    # Concatenate all batches\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "    # To inverse transform, we need to reconstruct a dummy array with the same shape\n",
        "    # as the one the scaler was fit on: (num_samples, num_features * num_locations)\n",
        "    # Flatten the predictions and labels\n",
        "    all_preds_flat = all_preds.flatten()\n",
        "    all_labels_flat = all_labels.flatten()\n",
        "\n",
        "    # Create dummy arrays for inverse scaling\n",
        "    dummy_preds_array = np.zeros((len(all_preds_flat), num_features * num_locations))\n",
        "    dummy_labels_array = np.zeros((len(all_labels_flat), num_features * num_locations))\n",
        "\n",
        "    # Place the predictions/labels into the correct column for the target feature\n",
        "    # The scaler expects data in the order of (feature1_loc1, feature1_loc2, ..., feature2_loc1, ...)\n",
        "    # The target feature is PM2.5, which is at index `target_feature_idx` in our feature list\n",
        "    num_total_series = num_features * num_locations\n",
        "    target_cols_indices = [i for i in range(num_total_series) if (i % num_features) == target_feature_idx]\n",
        "\n",
        "    # This is a bit complex, but necessary for the scaler. It places the flattened predictions\n",
        "    # back into the columns that correspond to the PM2.5 data for each location.\n",
        "    dummy_preds_array = np.zeros((all_preds.shape[0], num_total_series))\n",
        "    dummy_labels_array = np.zeros((all_labels.shape[0], num_total_series))\n",
        "\n",
        "    # Reshape the predictions and labels to match the number of samples and locations\n",
        "    all_preds_reshaped = all_preds.reshape(-1, num_locations)\n",
        "    all_labels_reshaped = all_labels.reshape(-1, num_locations)\n",
        "\n",
        "    # Assign the reshaped data to the appropriate columns in the dummy arrays\n",
        "    for i in range(num_locations):\n",
        "         dummy_preds_array[:, target_feature_idx + i * num_features] = all_preds_reshaped[:, i]\n",
        "         dummy_labels_array[:, target_feature_idx + i * num_features] = all_labels_reshaped[:, i]\n",
        "\n",
        "    # Inverse transform\n",
        "    preds_rescaled_full = scaler_obj.inverse_transform(dummy_preds_array)\n",
        "    labels_rescaled_full = scaler_obj.inverse_transform(dummy_labels_array)\n",
        "\n",
        "    # Extract only the columns corresponding to the target feature\n",
        "    preds_rescaled = preds_rescaled_full[:, target_feature_idx::num_features].flatten()\n",
        "    labels_rescaled = labels_rescaled_full[:, target_feature_idx::num_features].flatten()\n",
        "\n",
        "    # Calculate metrics\n",
        "    mae = np.mean(np.abs(preds_rescaled - labels_rescaled))\n",
        "    rmse = np.sqrt(np.mean((preds_rescaled - labels_rescaled)**2))\n",
        "\n",
        "    # --- ADDED ACCURACY METRIC (R-squared) ---\n",
        "    # It measures how well the model predictions explain the variance of the true values.\n",
        "    # A score of 1.0 is a perfect prediction.\n",
        "    ss_res = np.sum((labels_rescaled - preds_rescaled) ** 2)\n",
        "    ss_tot = np.sum((labels_rescaled - np.mean(labels_rescaled)) ** 2)\n",
        "    # Add a check to prevent division by zero if all target values are the same\n",
        "    if ss_tot == 0:\n",
        "        r2 = 0.0 # Or handle as an undefined case\n",
        "    else:\n",
        "        r2 = 1 - (ss_res / ss_tot)\n",
        "\n",
        "    return mae, rmse, r2\n",
        "\n",
        "\n",
        "# --- 4. Main Execution Block ---\n",
        "# This block assumes the variables from the previous preprocessing script are available\n",
        "# (X, y, edge_idx, data_scaler, n_locs, n_feats, final_feats_list)\n",
        "\n",
        "try:\n",
        "    # Check if the required variables from preprocessing exist\n",
        "    X, y, edge_idx, data_scaler, n_locs, n_feats, final_feats_list\n",
        "except NameError:\n",
        "    print(\"Error: Preprocessing variables (X, y, etc.) not found.\")\n",
        "    print(\"Please run the data processing script cell before this one.\")\n",
        "else:\n",
        "    # Setup device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Split data: 80% train, 10% validation, 10% test\n",
        "    train_size = int(0.8 * len(X))\n",
        "    val_size = int(0.1 * len(X))\n",
        "    test_size = len(X) - train_size - val_size\n",
        "\n",
        "    X_train, y_train = X[:train_size], y[:train_size]\n",
        "    X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
        "    X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
        "\n",
        "    # Create DataLoaders\n",
        "    batch_size = 32\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_dataset = TensorDataset(X_val, y_val)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Instantiate the model using the variables from your preprocessing\n",
        "    model = PCDCNet(num_features=n_feats, num_locations=n_locs).to(device)\n",
        "    edge_idx = edge_idx.to(device)\n",
        "    print(\"\\n--- Model Architecture ---\")\n",
        "    print(model)\n",
        "    print(\"--------------------------\\n\")\n",
        "\n",
        "    # Train the model\n",
        "    train_losses, val_losses = train_model(model, train_loader, val_loader, edge_idx, device, epochs=50)\n",
        "\n",
        "    # Plot training and validation loss\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Training & Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Evaluate the final model on the test set\n",
        "    target_feature_idx = final_feats_list.index('Daily Mean PM2.5 Concentration')\n",
        "    # --- UPDATED TO RECEIVE r2 ---\n",
        "    mae, rmse, r2 = evaluate_model(model, test_loader, edge_idx, device, data_scaler, n_locs, n_feats, target_feature_idx)\n",
        "\n",
        "    # --- UPDATED TO PRINT r2 ---\n",
        "    print(f\"\\n--- Final Evaluation Metrics ---\")\n",
        "    print(f\"Test MAE (Mean Absolute Error): {mae:.4f}\")\n",
        "    print(f\"Test RMSE (Root Mean Squared Error): {rmse:.4f}\")\n",
        "    print(f\"Test R-squared (Accuracy): {r2:.4f}\")\n",
        "    print(\"--------------------------------\")\n"
      ],
      "metadata": {
        "id": "_tK-K-mcIqNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VahoPLoKrLlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCDC Using LSTM"
      ],
      "metadata": {
        "id": "vuFr6Pddk5yw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. PCDCNet Model Definition (Now with LSTM) ---\n",
        "class PCDCNet(nn.Module):\n",
        "    def __init__(self, num_features, num_locations, hidden_dim=64, gcn_out_dim=32):\n",
        "        super(PCDCNet, self).__init__()\n",
        "\n",
        "        # MLP for local feature enhancement\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(num_features, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Graph Convolutional Network for spatial dependencies\n",
        "        self.gcn = GCNConv(hidden_dim, gcn_out_dim)\n",
        "\n",
        "        # --- MODIFICATION: Using LSTM instead of GRU ---\n",
        "        # The LSTM layer has the same parameters for this use case.\n",
        "        self.lstm = nn.LSTM(gcn_out_dim, hidden_dim, batch_first=True)\n",
        "        # --- END OF MODIFICATION ---\n",
        "\n",
        "        # Output layer\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # x shape: (batch_size, lookback, num_locations, num_features)\n",
        "        batch_size, lookback, num_loc, num_feat = x.shape\n",
        "\n",
        "        # Process each time step\n",
        "        lstm_inputs = []\n",
        "        for t in range(lookback):\n",
        "            # Current time step data for all locations in the batch\n",
        "            x_t = x[:, t, :, :]\n",
        "            x_t = x_t.reshape(batch_size * num_loc, num_feat)\n",
        "\n",
        "            # Local feature enhancement with MLP\n",
        "            mlp_out = self.mlp(x_t)\n",
        "\n",
        "            # Reshape for GCN\n",
        "            mlp_out = mlp_out.reshape(batch_size, num_loc, -1)\n",
        "\n",
        "            # GCN for spatial modeling\n",
        "            gcn_batch_out = []\n",
        "            for i in range(batch_size):\n",
        "                gcn_out = self.gcn(mlp_out[i], edge_index)\n",
        "                gcn_batch_out.append(gcn_out)\n",
        "\n",
        "            gcn_out_tensor = torch.stack(gcn_batch_out)\n",
        "            lstm_inputs.append(gcn_out_tensor)\n",
        "\n",
        "        # Stack to create a sequence for the LSTM\n",
        "        lstm_input_seq = torch.stack(lstm_inputs, dim=1)\n",
        "        lstm_input_seq = lstm_input_seq.reshape(batch_size * num_loc, lookback, -1)\n",
        "\n",
        "        # --- MODIFICATION: Calling the LSTM layer ---\n",
        "        # The LSTM returns the output and a tuple of (hidden_state, cell_state).\n",
        "        # We only need the main output sequence for this logic.\n",
        "        lstm_out, _ = self.lstm(lstm_input_seq)\n",
        "        # --- END OF MODIFICATION ---\n",
        "\n",
        "        # Use the output of the last time step\n",
        "        last_hidden_state = lstm_out[:, -1, :]\n",
        "\n",
        "        # Reshape back to (batch_size, num_loc, hidden_dim)\n",
        "        last_hidden_state = last_hidden_state.reshape(batch_size, num_loc, -1)\n",
        "\n",
        "        # Fully connected layer for the final prediction\n",
        "        output = self.fc(last_hidden_state)\n",
        "\n",
        "        return output.squeeze(-1)\n",
        "\n",
        "\n",
        "# --- 2. Model Training Function (No Changes Needed) ---\n",
        "def train_model(model, train_loader, val_loader, edge_index, device, epochs=50, learning_rate=0.001):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    print(\"\\n--- Starting Model Training ---\")\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs, edge_index)\n",
        "            loss = criterion(outputs, labels)\n",
        "            if torch.isnan(loss):\n",
        "                print(f\"NaN detected in loss at epoch {epoch+1}. Stopping training.\")\n",
        "                return train_losses, val_losses\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        val_running_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs, edge_index)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_running_loss += loss.item()\n",
        "        val_loss = val_running_loss / len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
        "\n",
        "    print(\"--- Model Training Complete ---\")\n",
        "    return train_losses, val_losses\n",
        "\n",
        "\n",
        "# --- 3. Model Evaluation Function (No Changes Needed) ---\n",
        "def evaluate_model(model, test_loader, edge_index, device, scaler_obj, num_locations, num_features, target_feature_idx):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    print(\"\\n--- Evaluating Model on Test Set ---\")\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs, edge_index)\n",
        "            all_preds.append(outputs.cpu().numpy())\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    num_total_original_features = scaler_obj.n_features_in_\n",
        "    dummy_preds_array = np.zeros((all_preds.shape[0], num_total_original_features))\n",
        "    dummy_labels_array = np.zeros((all_labels.shape[0], num_total_original_features))\n",
        "    start_col_idx = target_feature_idx * num_locations\n",
        "    end_col_idx = start_col_idx + num_locations\n",
        "    dummy_preds_array[:, start_col_idx:end_col_idx] = all_preds\n",
        "    dummy_labels_array[:, start_col_idx:end_col_idx] = all_labels\n",
        "    preds_rescaled_full = scaler_obj.inverse_transform(dummy_preds_array)\n",
        "    labels_rescaled_full = scaler_obj.inverse_transform(dummy_labels_array)\n",
        "    preds_rescaled = preds_rescaled_full[:, start_col_idx:end_col_idx].flatten()\n",
        "    labels_rescaled = labels_rescaled_full[:, start_col_idx:end_col_idx].flatten()\n",
        "    mae = np.mean(np.abs(preds_rescaled - labels_rescaled))\n",
        "    rmse = np.sqrt(np.mean((preds_rescaled - labels_rescaled)**2))\n",
        "    ss_res = np.sum((labels_rescaled - preds_rescaled)**2)\n",
        "    ss_tot = np.sum((labels_rescaled - np.mean(labels_rescaled))**2)\n",
        "    r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0.0\n",
        "    return mae, rmse, r2\n",
        "\n",
        "\n",
        "# --- 4. Main Execution Block (No Changes Needed) ---\n",
        "# This block assumes the variables from your preprocessing script are available\n",
        "try:\n",
        "    X, y, edge_idx, data_scaler, n_locs, n_feats, final_feats_list\n",
        "except NameError:\n",
        "    print(\"Error: Preprocessing variables (X, y, etc.) not found.\")\n",
        "    print(\"Please run the data processing script cell before this one.\")\n",
        "else:\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Data splitting\n",
        "    train_size = int(0.8 * len(X))\n",
        "    val_size = int(0.1 * len(X))\n",
        "    test_size = len(X) - train_size - val_size\n",
        "    X_train, y_train = X[:train_size], y[:train_size]\n",
        "    X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
        "    X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
        "\n",
        "    # DataLoaders\n",
        "    batch_size = 32\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_dataset = TensorDataset(X_val, y_val)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Instantiate the model with the LSTM layer\n",
        "    model = PCDCNet(num_features=n_feats, num_locations=n_locs).to(device)\n",
        "    edge_idx = edge_idx.to(device)\n",
        "    print(\"\\n--- Model Architecture (LSTM Version) ---\")\n",
        "    print(model)\n",
        "    print(\"---------------------------------------\\n\")\n",
        "\n",
        "    # Train the model\n",
        "    train_losses, val_losses = train_model(model, train_loader, val_loader, edge_idx, device, epochs=50)\n",
        "\n",
        "    # Plot loss\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Training & Validation Loss (LSTM Model)')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Evaluate the final model\n",
        "    target_feature_idx = final_feats_list.index('Daily Mean PM2.5 Concentration')\n",
        "    mae, rmse, r2 = evaluate_model(model, test_loader, edge_idx, device, data_scaler, n_locs, n_feats, target_feature_idx)\n",
        "\n",
        "    print(f\"\\n--- Final Evaluation Metrics (LSTM Model) ---\")\n",
        "    print(f\"Test MAE (Mean Absolute Error): {mae:.4f}\")\n",
        "    print(f\"Test RMSE (Root Mean Squared Error): {rmse:.4f}\")\n",
        "    print(f\"Test R-squared (Accuracy): {r2:.4f}\")\n",
        "    print(\"-------------------------------------------\")\n"
      ],
      "metadata": {
        "id": "UjPnsuAZk8VX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# --- This assumes these variables exist in your current Colab session after training ---\n",
        "# model: your trained PCDCNet model\n",
        "# X, y, edge_idx: Tensors from the preprocessing step\n",
        "# data_scaler: The StandardScaler object from preprocessing\n",
        "# n_locs, n_feats, final_feats_list: Variables from preprocessing\n",
        "\n",
        "# Define the paths to save the files in your 'pcdc' folder\n",
        "save_directory = '/content/drive/MyDrive/pcdc/'\n",
        "model_path = os.path.join(save_directory, 'pcdcnet_model.pth')\n",
        "data_path = os.path.join(save_directory, 'evaluation_data.pt')\n",
        "scaler_path = os.path.join(save_directory, 'scaler.gz')\n",
        "\n",
        "print(\"--- Saving model and CORRECT evaluation data ---\")\n",
        "\n",
        "# 1. Save the trained model's state dictionary\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to: {model_path}\")\n",
        "\n",
        "# 2. Save the data scaler\n",
        "joblib.dump(data_scaler, scaler_path)\n",
        "print(f\"Scaler saved to: {scaler_path}\")\n",
        "\n",
        "# 3. Save all other necessary tensors and variables\n",
        "evaluation_data = {\n",
        "    'X': X,\n",
        "    'y': y,\n",
        "    'edge_idx': edge_idx,\n",
        "    'n_locs': n_locs,\n",
        "    'n_feats': n_feats,\n",
        "    'final_feats_list': final_feats_list\n",
        "}\n",
        "torch.save(evaluation_data, data_path)\n",
        "print(f\"Evaluation data saved to: {data_path}\")\n",
        "\n",
        "print(\"--- All files have been re-saved and are now synchronized! ---\")"
      ],
      "metadata": {
        "id": "wnYuQMUssGzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# --- PCDCNet Model Definition (Using GRU to match your saved model) ---\n",
        "class PCDCNet(nn.Module):\n",
        "    def __init__(self, num_features, num_locations, hidden_dim=64, gcn_out_dim=32):\n",
        "        super(PCDCNet, self).__init__()\n",
        "        self.mlp = nn.Sequential(nn.Linear(num_features, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))\n",
        "        self.gcn = GCNConv(hidden_dim, gcn_out_dim)\n",
        "        # --- Using GRU to match the architecture of the saved model weights ---\n",
        "        self.gru = nn.GRU(gcn_out_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        batch_size, lookback, num_loc, num_feat = x.shape\n",
        "        recurrent_inputs = []\n",
        "        for t in range(lookback):\n",
        "            x_t = x[:, t, :, :].reshape(batch_size * num_loc, num_feat)\n",
        "            mlp_out = self.mlp(x_t).reshape(batch_size, num_loc, -1)\n",
        "            gcn_batch_out = []\n",
        "            for i in range(batch_size):\n",
        "                gcn_batch_out.append(self.gcn(mlp_out[i], edge_index))\n",
        "            recurrent_inputs.append(torch.stack(gcn_batch_out))\n",
        "\n",
        "        recurrent_input_seq = torch.stack(recurrent_inputs, dim=1).reshape(batch_size * num_loc, lookback, -1)\n",
        "        # --- Calling the GRU layer ---\n",
        "        recurrent_out, _ = self.gru(recurrent_input_seq)\n",
        "        last_hidden_state = recurrent_out[:, -1, :].reshape(batch_size, num_loc, -1)\n",
        "        output = self.fc(last_hidden_state)\n",
        "        return output.squeeze(-1)\n",
        "\n",
        "# --- CORRECTED Model Evaluation Function ---\n",
        "def evaluate_model(model, test_loader, edge_index, device, scaler_obj, num_locations, num_features, target_feature_idx):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    print(\"\\n--- Evaluating Model on Test Set ---\")\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs, edge_index)\n",
        "            all_preds.append(outputs.cpu().numpy())\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "    # --- ADDED ROBUSTNESS CHECK ---\n",
        "    # Infer the number of locations directly from the model's output shape\n",
        "    n_locs_from_model = all_preds.shape[1]\n",
        "\n",
        "    # Check for a mismatch between the model's output and the loaded data parameters\n",
        "    if n_locs_from_model != num_locations:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"!! FATAL ERROR: Data and Model Mismatch !!\")\n",
        "        print(f\"The loaded model is making predictions for {n_locs_from_model} locations.\")\n",
        "        print(f\"However, the loaded data/scaler object is for {num_locations} locations.\")\n",
        "        print(\"This means the saved model file and the saved data/scaler files are out of sync.\")\n",
        "        print(\"\\nSOLUTION: Please re-run the 'Save Model & Data' script in the\")\n",
        "        print(\"Colab notebook session where you successfully trained the model.\")\n",
        "        print(\"=\"*60 + \"\\n\")\n",
        "        return None, None, None # Stop evaluation\n",
        "    # --- END OF CHECK ---\n",
        "\n",
        "    num_total_original_features = scaler_obj.n_features_in_\n",
        "    dummy_preds_array = np.zeros((all_preds.shape[0], num_total_original_features))\n",
        "    dummy_labels_array = np.zeros((all_labels.shape[0], num_total_original_features))\n",
        "\n",
        "    start_col_idx = target_feature_idx * num_locations\n",
        "    end_col_idx = start_col_idx + num_locations\n",
        "\n",
        "    dummy_preds_array[:, start_col_idx:end_col_idx] = all_preds\n",
        "    dummy_labels_array[:, start_col_idx:end_col_idx] = all_labels\n",
        "\n",
        "    preds_rescaled_full = scaler_obj.inverse_transform(dummy_preds_array)\n",
        "    labels_rescaled_full = scaler_obj.inverse_transform(dummy_labels_array)\n",
        "\n",
        "    preds_rescaled = preds_rescaled_full[:, start_col_idx:end_col_idx].flatten()\n",
        "    labels_rescaled = labels_rescaled_full[:, start_col_idx:end_col_idx].flatten()\n",
        "\n",
        "    mae = np.mean(np.abs(preds_rescaled - labels_rescaled))\n",
        "    rmse = np.sqrt(np.mean((preds_rescaled - labels_rescaled)**2))\n",
        "    ss_res = np.sum((labels_rescaled - preds_rescaled)**2)\n",
        "    ss_tot = np.sum((labels_rescaled - np.mean(labels_rescaled))**2)\n",
        "    r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0.0\n",
        "    return mae, rmse, r2\n",
        "\n",
        "# --- Main Evaluation Script ---\n",
        "# Define paths to the saved files in your Google Drive\n",
        "load_directory = '/content/drive/MyDrive/pcdc/'\n",
        "model_path = os.path.join(load_directory, 'pcdcnet_model.pth')\n",
        "data_path = os.path.join(load_directory, 'evaluation_data.pt')\n",
        "scaler_path = os.path.join(load_directory, 'scaler.gz')\n",
        "\n",
        "# Check if all files exist before proceeding\n",
        "if not all(os.path.exists(p) for p in [model_path, data_path, scaler_path]):\n",
        "    print(\"Error: One or more required files are missing.\")\n",
        "    print(\"Please ensure you have run a script to save the trained model, scaler, and data tensors.\")\n",
        "else:\n",
        "    # Setup device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # 1. Load the data objects\n",
        "    eval_data = torch.load(data_path)\n",
        "    X = eval_data['X']\n",
        "    y = eval_data['y']\n",
        "    edge_idx = eval_data['edge_idx'].to(device)\n",
        "    n_locs = eval_data['n_locs']\n",
        "    n_feats = eval_data['n_feats']\n",
        "    final_feats_list = eval_data['final_feats_list']\n",
        "    print(\"Loaded evaluation data and tensors.\")\n",
        "\n",
        "    # 2. Load the data scaler\n",
        "    data_scaler = joblib.load(scaler_path)\n",
        "    print(\"Loaded data scaler.\")\n",
        "\n",
        "    # 3. Instantiate the GRU model and load the saved weights\n",
        "    model = PCDCNet(num_features=n_feats, num_locations=n_locs).to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    print(\"Instantiated GRU model and loaded trained weights.\")\n",
        "\n",
        "    # 4. Recreate the test data split and DataLoader\n",
        "    train_size = int(0.8 * len(X))\n",
        "    val_size = int(0.1 * len(X))\n",
        "    X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "    print(\"Recreated test data loader.\")\n",
        "\n",
        "    # 5. Run the evaluation\n",
        "    target_feature_idx = final_feats_list.index('Daily Mean PM2.5 Concentration')\n",
        "    mae, rmse, r2 = evaluate_model(model, test_loader, edge_idx, device, data_scaler, n_locs, n_feats, target_feature_idx)\n",
        "\n",
        "    # 6. Print the final metrics\n",
        "    if mae is not None: # Check if evaluation was successful\n",
        "        print(f\"\\n--- Final Evaluation Metrics ---\")\n",
        "        print(f\"Test MAE (Mean Absolute Error): {mae:.4f}\")\n",
        "        print(f\"Test RMSE (Root Mean Squared Error): {rmse:.4f}\")\n",
        "        print(f\"Test R-squared (Accuracy): {r2:.4f}\")\n",
        "        print(\"--------------------------------\")\n"
      ],
      "metadata": {
        "id": "uw8jGVlfsZUt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}